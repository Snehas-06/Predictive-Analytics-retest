# -*- coding: utf-8 -*-
"""LVADSUSR121_CLUSTERING_LAB3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b-ATIE0QyNHYPgfj8r9MtfMQEQ7PxESB
"""

#importing libraries and reading data
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.impute import SimpleImputer
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
df3 = pd.read_csv('/content/drive/MyDrive/Predictive analytics Practice/customer_segmentation.csv')
print(df3)

df3.info()

# Check for missing values and replacing it
print("Missing Values Before Imputation:")
print(df3.isnull().sum())

#Label encoding
# Initialize LabelEncoder
label_encoder = LabelEncoder()

# List of categorical columns to encode
categorical_columns = ['Education', 'Marital_Status', 'Dt_Customer']

# Apply label encoding to each categorical column
for col in categorical_columns:
    df3[col] = label_encoder.fit_transform(df3[col])

#updated DataFrame
print(df3)

# Replacing missing values with median
imputer = SimpleImputer(strategy='median')
df_Imputed = pd.DataFrame(imputer.fit_transform(df3), columns=df3.columns)

# Step 3: Remove Outliers
# Use Isolation Forest to identify outliers
outliers_detector = IsolationForest(contamination=0.05, random_state=42)
outliers = outliers_detector.fit_predict(df_Imputed)

# Removing outliers
df_cleaned = df_Imputed[outliers == 1]

# Step 4: Perform Exploratory Data Analysis (EDA)
# Summary Statistics
print("Summary Statistics:")
print(df_cleaned.describe())

#Visualization
sns.pairplot(df_cleaned)
plt.show()

#Model Training and Testing
# Feature Scaling
scaler = StandardScaler()
X = scaler.fit_transform(df_cleaned)

# Splitting into training and test sets
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)

# Fit KMeans clustering model on training set
kmeans = KMeans(random_state=42)

# Initialize list to store inertia values
inertia_values = []

# Range of clusters to test
no_clusters_range = range(1, 11)

# Calculate inertia for each number of clusters
for no_clusters in no_clusters_range:
    kmeans.set_params(n_clusters=no_clusters)
    kmeans.fit(X_train)
    inertia_values.append(kmeans.inertia_)

# Plotting the Elbow Method
plt.plot(no_clusters_range, inertia_values, marker='o', linestyle='-')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.xticks(no_clusters_range)
plt.show()

#Model Evaluation (Optional)
# Evaluate clustering using silhouette score on test set
silhouette_avg = silhouette_score(X_test, kmeans.predict(X_test))
print("Silhouette Score:", silhouette_avg)

#Prediction on Test Set
# If there is a separate test dataset, it can be read it similarly like the training data
# Or we can use the existing dataset and split it into training and test sets
# Then, we can predict clusters on the test set using the trained model

# For example, if we have df_test as our test dataset
# Predict clusters on the test set
test_clusters = kmeans.predict(X_test)

# Print the predicted clusters on the test set
print("Predicted Clusters on Test Set:")
print(test_clusters)



"""Finally this model groups the customers and can help business provide strategy techniques to increase sales"""